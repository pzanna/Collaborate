"""
Advanced Classification Models for Systematic Reviews - Phase 4A Implementation.

This module provides next-generation AI/ML models for enhanced study classification,
multi-modal analysis, and continuous learning capabilities.
"""

import json
import logging
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import hashlib
from pathlib import Path


class ModelType(Enum):
    """Types of classification models."""
    TRANSFORMER = "transformer"
    ENSEMBLE = "ensemble"
    NEURAL_NETWORK = "neural_network"
    GRADIENT_BOOSTING = "gradient_boosting"
    SVM = "svm"
    HYBRID = "hybrid"


class ConfidenceLevel(Enum):
    """Confidence levels for predictions."""
    VERY_HIGH = "very_high"  # >95%
    HIGH = "high"            # 85-95%
    MODERATE = "moderate"    # 70-85%
    LOW = "low"              # 55-70%
    VERY_LOW = "very_low"    # <55%


@dataclass
class ClassificationFeatures:
    """Features extracted from study for classification."""
    title_features: Dict[str, Any]
    abstract_features: Dict[str, Any]
    author_features: Dict[str, Any]
    journal_features: Dict[str, Any]
    metadata_features: Dict[str, Any]
    linguistic_features: Dict[str, Any]
    semantic_features: Dict[str, Any]
    
    def to_vector(self) -> np.ndarray:
        """Convert features to numerical vector for ML models."""
        # This would implement feature vectorization
        # For demonstration, return mock vector
        return np.random.random(256)


@dataclass
class ClassificationResult:
    """Enhanced classification result with confidence and explanations."""
    study_id: str
    predicted_class: str
    confidence_score: float
    confidence_level: ConfidenceLevel
    class_probabilities: Dict[str, float]
    feature_importance: Dict[str, float]
    explanation: str
    model_version: str
    prediction_timestamp: str
    uncertainty_measures: Dict[str, float]
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return {
            'study_id': self.study_id,
            'predicted_class': self.predicted_class,
            'confidence_score': self.confidence_score,
            'confidence_level': self.confidence_level.value,
            'class_probabilities': self.class_probabilities,
            'feature_importance': self.feature_importance,
            'explanation': self.explanation,
            'model_version': self.model_version,
            'prediction_timestamp': self.prediction_timestamp,
            'uncertainty_measures': self.uncertainty_measures
        }


@dataclass
class ModelPerformance:
    """Model performance metrics and validation results."""
    model_id: str
    accuracy: float
    precision: Dict[str, float]
    recall: Dict[str, float]
    f1_score: Dict[str, float]
    auc_roc: float
    confusion_matrix: List[List[int]]
    calibration_score: float
    uncertainty_quality: float
    validation_date: str
    test_dataset_info: Dict[str, Any]
    
    def get_overall_score(self) -> float:
        """Calculate overall model performance score."""
        precision_avg = sum(self.precision.values()) / len(self.precision)
        recall_avg = sum(self.recall.values()) / len(self.recall)
        f1_avg = sum(self.f1_score.values()) / len(self.f1_score)
        
        return (self.accuracy * 0.3 + 
                precision_avg * 0.25 + 
                recall_avg * 0.25 + 
                f1_avg * 0.2)


class AdvancedClassificationModel:
    """
    Base class for advanced classification models.
    
    Provides common interface for different model types including
    transformers, ensembles, and hybrid approaches.
    """
    
    def __init__(self, model_id: str, model_type: ModelType, config: Dict[str, Any]):
        """Initialize the classification model."""
        self.model_id = model_id
        self.model_type = model_type
        self.config = config
        self.model = None
        self.is_trained = False
        self.performance_history = []
        self.logger = logging.getLogger(__name__)
    
    async def train(self, training_data: List[Dict[str, Any]], validation_data: List[Dict[str, Any]]) -> ModelPerformance:
        """Train the model on provided data."""
        raise NotImplementedError("Subclasses must implement train method")
    
    async def predict(self, features: ClassificationFeatures) -> ClassificationResult:
        """Make prediction on study features."""
        raise NotImplementedError("Subclasses must implement predict method")
    
    async def predict_batch(self, features_list: List[ClassificationFeatures]) -> List[ClassificationResult]:
        """Make batch predictions for multiple studies."""
        results = []
        for features in features_list:
            result = await self.predict(features)
            results.append(result)
        return results
    
    def save_model(self, path: str) -> None:
        """Save trained model to disk."""
        raise NotImplementedError("Subclasses must implement save_model method")
    
    def load_model(self, path: str) -> None:
        """Load trained model from disk."""
        raise NotImplementedError("Subclasses must implement load_model method")


class TransformerClassificationModel(AdvancedClassificationModel):
    """
    Transformer-based classification model using pre-trained language models.
    
    Leverages BERT, RoBERTa, or similar models for sophisticated text understanding
    and classification of study designs.
    """
    
    def __init__(self, model_id: str, config: Dict[str, Any]):
        """Initialize transformer model."""
        super().__init__(model_id, ModelType.TRANSFORMER, config)
        self.base_model = config.get('base_model', 'bert-base-uncased')
        self.max_length = config.get('max_length', 512)
        self.num_classes = config.get('num_classes', 13)
        self.fine_tune_layers = config.get('fine_tune_layers', 2)
    
    async def train(self, training_data: List[Dict[str, Any]], validation_data: List[Dict[str, Any]]) -> ModelPerformance:
        """Train transformer model with fine-tuning."""
        self.logger.info(f"Training transformer model {self.model_id} on {len(training_data)} samples")
        
        # Simulate training process
        # In production, this would use HuggingFace Transformers or similar
        await asyncio.sleep(2)  # Simulate training time
        
        # Mock performance metrics
        performance = ModelPerformance(
            model_id=self.model_id,
            accuracy=0.947,
            precision={'RCT': 0.952, 'Cohort': 0.934, 'Case-Control': 0.941, 'Cross-sectional': 0.953},
            recall={'RCT': 0.948, 'Cohort': 0.942, 'Case-Control': 0.935, 'Cross-sectional': 0.951},
            f1_score={'RCT': 0.950, 'Cohort': 0.938, 'Case-Control': 0.938, 'Cross-sectional': 0.952},
            auc_roc=0.987,
            confusion_matrix=[[195, 3, 1, 1], [2, 188, 5, 5], [1, 4, 187, 8], [0, 2, 3, 195]],
            calibration_score=0.923,
            uncertainty_quality=0.856,
            validation_date=datetime.now().isoformat(),
            test_dataset_info={'size': len(validation_data), 'classes': self.num_classes}
        )
        
        self.is_trained = True
        self.performance_history.append(performance)
        
        self.logger.info(f"Training completed. Accuracy: {performance.accuracy:.3f}")
        return performance
    
    async def predict(self, features: ClassificationFeatures) -> ClassificationResult:
        """Make prediction using transformer model."""
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        # Simulate transformer prediction
        # In production, this would use the actual model
        class_names = ['RCT', 'Cohort', 'Case-Control', 'Cross-sectional', 'Qualitative', 'Mixed-Methods']
        probabilities = np.random.dirichlet(np.ones(len(class_names)))
        predicted_idx = np.argmax(probabilities)
        predicted_class = class_names[predicted_idx]
        confidence = float(probabilities[predicted_idx])
        
        # Determine confidence level
        if confidence >= 0.95:
            conf_level = ConfidenceLevel.VERY_HIGH
        elif confidence >= 0.85:
            conf_level = ConfidenceLevel.HIGH
        elif confidence >= 0.70:
            conf_level = ConfidenceLevel.MODERATE
        elif confidence >= 0.55:
            conf_level = ConfidenceLevel.LOW
        else:
            conf_level = ConfidenceLevel.VERY_LOW
        
        # Generate explanation
        explanation = f"Classified as {predicted_class} based on transformer analysis of title and abstract. Key indicators include study design terminology, methodology descriptions, and statistical analysis mentions."
        
        return ClassificationResult(
            study_id=f"study_{hash(str(features))%10000}",
            predicted_class=predicted_class,
            confidence_score=confidence,
            confidence_level=conf_level,
            class_probabilities=dict(zip(class_names, probabilities.tolist())),
            feature_importance={
                'title_keywords': 0.35,
                'methodology_terms': 0.28,
                'statistical_terms': 0.22,
                'journal_context': 0.15
            },
            explanation=explanation,
            model_version=self.model_id,
            prediction_timestamp=datetime.now().isoformat(),
            uncertainty_measures={
                'epistemic': np.random.uniform(0.1, 0.3),
                'aleatoric': np.random.uniform(0.05, 0.2)
            }
        )


class EnsembleClassificationModel(AdvancedClassificationModel):
    """
    Ensemble classification model combining multiple approaches.
    
    Combines transformer models, gradient boosting, and rule-based classifiers
    for robust and accurate study design classification.
    """
    
    def __init__(self, model_id: str, config: Dict[str, Any]):
        """Initialize ensemble model."""
        super().__init__(model_id, ModelType.ENSEMBLE, config)
        self.base_models = []
        self.ensemble_method = config.get('ensemble_method', 'weighted_voting')
        self.model_weights = config.get('model_weights', None)
    
    async def train(self, training_data: List[Dict[str, Any]], validation_data: List[Dict[str, Any]]) -> ModelPerformance:
        """Train ensemble of models."""
        self.logger.info(f"Training ensemble model {self.model_id} with {len(self.config.get('base_model_configs', []))} base models")
        
        # Simulate training multiple models
        await asyncio.sleep(3)  # Simulate longer training time for ensemble
        
        # Mock superior performance from ensemble
        performance = ModelPerformance(
            model_id=self.model_id,
            accuracy=0.963,
            precision={'RCT': 0.968, 'Cohort': 0.954, 'Case-Control': 0.961, 'Cross-sectional': 0.969},
            recall={'RCT': 0.965, 'Cohort': 0.958, 'Case-Control': 0.956, 'Cross-sectional': 0.967},
            f1_score={'RCT': 0.966, 'Cohort': 0.956, 'Case-Control': 0.958, 'Cross-sectional': 0.968},
            auc_roc=0.994,
            confusion_matrix=[[197, 2, 1, 0], [1, 191, 3, 5], [0, 3, 191, 6], [0, 1, 2, 197]],
            calibration_score=0.951,
            uncertainty_quality=0.892,
            validation_date=datetime.now().isoformat(),
            test_dataset_info={'size': len(validation_data), 'classes': 13}
        )
        
        self.is_trained = True
        self.performance_history.append(performance)
        
        self.logger.info(f"Ensemble training completed. Accuracy: {performance.accuracy:.3f}")
        return performance
    
    async def predict(self, features: ClassificationFeatures) -> ClassificationResult:
        """Make ensemble prediction."""
        if not self.is_trained:
            raise ValueError("Ensemble model must be trained before making predictions")
        
        # Simulate ensemble prediction with higher confidence
        class_names = ['RCT', 'Cohort', 'Case-Control', 'Cross-sectional', 'Qualitative', 'Mixed-Methods']
        
        # Ensemble typically has higher confidence and better calibration
        base_probabilities = np.random.dirichlet(np.ones(len(class_names)) * 2)  # More concentrated
        probabilities = base_probabilities / np.sum(base_probabilities)
        
        predicted_idx = np.argmax(probabilities)
        predicted_class = class_names[predicted_idx]
        confidence = float(probabilities[predicted_idx])
        
        # Ensemble models typically have better confidence estimation
        if confidence >= 0.90:
            conf_level = ConfidenceLevel.VERY_HIGH
        elif confidence >= 0.80:
            conf_level = ConfidenceLevel.HIGH
        elif confidence >= 0.65:
            conf_level = ConfidenceLevel.MODERATE
        elif confidence >= 0.50:
            conf_level = ConfidenceLevel.LOW
        else:
            conf_level = ConfidenceLevel.VERY_LOW
        
        explanation = f"Ensemble classification as {predicted_class} with high confidence. Multiple models (transformer, gradient boosting, rule-based) reached consensus on classification based on comprehensive feature analysis."
        
        return ClassificationResult(
            study_id=f"study_{hash(str(features))%10000}",
            predicted_class=predicted_class,
            confidence_score=confidence,
            confidence_level=conf_level,
            class_probabilities=dict(zip(class_names, probabilities.tolist())),
            feature_importance={
                'linguistic_patterns': 0.32,
                'methodology_indicators': 0.29,
                'statistical_terminology': 0.21,
                'structural_features': 0.18
            },
            explanation=explanation,
            model_version=self.model_id,
            prediction_timestamp=datetime.now().isoformat(),
            uncertainty_measures={
                'model_disagreement': np.random.uniform(0.05, 0.15),
                'prediction_entropy': np.random.uniform(0.1, 0.25)
            }
        )


class ModelManager:
    """
    Manages multiple classification models and provides unified interface.
    
    Handles model training, validation, selection, and deployment with
    A/B testing and performance monitoring capabilities.
    """
    
    def __init__(self, database: Any, config: Dict[str, Any]):
        """Initialize model manager."""
        self.database = database
        self.config = config
        self.models: Dict[str, AdvancedClassificationModel] = {}
        self.active_model_id: Optional[str] = None
        self.model_registry = {}
        self.logger = logging.getLogger(__name__)
    
    def register_model(self, model: AdvancedClassificationModel) -> None:
        """Register a new model with the manager."""
        self.models[model.model_id] = model
        self.model_registry[model.model_id] = {
            'model_type': model.model_type.value,
            'config': model.config,
            'registered_at': datetime.now().isoformat(),
            'is_trained': model.is_trained
        }
        self.logger.info(f"Registered model {model.model_id} of type {model.model_type.value}")
    
    async def train_model(self, model_id: str, training_data: List[Dict[str, Any]], validation_data: List[Dict[str, Any]]) -> ModelPerformance:
        """Train a specific model."""
        if model_id not in self.models:
            raise ValueError(f"Model {model_id} not found in registry")
        
        model = self.models[model_id]
        performance = await model.train(training_data, validation_data)
        
        # Store performance in database
        await self._store_model_performance(performance)
        
        return performance
    
    async def compare_models(self, model_ids: List[str], test_data: List[Dict[str, Any]]) -> Dict[str, ModelPerformance]:
        """Compare performance of multiple models on test data."""
        results = {}
        
        for model_id in model_ids:
            if model_id not in self.models:
                self.logger.warning(f"Model {model_id} not found, skipping comparison")
                continue
            
            model = self.models[model_id]
            if not model.is_trained:
                self.logger.warning(f"Model {model_id} not trained, skipping comparison")
                continue
            
            # Simulate model evaluation
            # In production, this would run actual evaluation
            await asyncio.sleep(1)
            
            # Generate mock performance based on model type
            if model.model_type == ModelType.ENSEMBLE:
                accuracy = np.random.uniform(0.92, 0.97)
            elif model.model_type == ModelType.TRANSFORMER:
                accuracy = np.random.uniform(0.88, 0.95)
            else:
                accuracy = np.random.uniform(0.80, 0.90)
            
            performance = ModelPerformance(
                model_id=model_id,
                accuracy=accuracy,
                precision={'avg': accuracy + np.random.uniform(-0.02, 0.02)},
                recall={'avg': accuracy + np.random.uniform(-0.02, 0.02)},
                f1_score={'avg': accuracy + np.random.uniform(-0.02, 0.02)},
                auc_roc=accuracy + np.random.uniform(0.01, 0.03),
                confusion_matrix=[],
                calibration_score=accuracy + np.random.uniform(-0.05, 0.02),
                uncertainty_quality=accuracy + np.random.uniform(-0.1, 0.05),
                validation_date=datetime.now().isoformat(),
                test_dataset_info={'size': len(test_data)}
            )
            
            results[model_id] = performance
        
        return results
    
    async def select_best_model(self, comparison_results: Dict[str, ModelPerformance]) -> str:
        """Select the best performing model based on comparison results."""
        best_model_id = None
        best_score = 0
        
        for model_id, performance in comparison_results.items():
            score = performance.get_overall_score()
            if score > best_score:
                best_score = score
                best_model_id = model_id
        
        if best_model_id:
            self.active_model_id = best_model_id
            self.logger.info(f"Selected {best_model_id} as active model with score {best_score:.3f}")
        
        return best_model_id
    
    async def predict_with_active_model(self, features: ClassificationFeatures) -> ClassificationResult:
        """Make prediction using the currently active model."""
        if not self.active_model_id:
            raise ValueError("No active model selected")
        
        if self.active_model_id not in self.models:
            raise ValueError(f"Active model {self.active_model_id} not found")
        
        model = self.models[self.active_model_id]
        return await model.predict(features)
    
    async def a_b_test_models(self, model_a_id: str, model_b_id: str, test_ratio: float = 0.5) -> Dict[str, Any]:
        """Perform A/B testing between two models."""
        self.logger.info(f"Starting A/B test between {model_a_id} and {model_b_id}")
        
        # Simulate A/B testing results
        # In production, this would collect real user feedback and performance metrics
        await asyncio.sleep(2)
        
        results = {
            'test_id': f"ab_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'model_a': {
                'model_id': model_a_id,
                'accuracy': np.random.uniform(0.85, 0.95),
                'user_satisfaction': np.random.uniform(3.5, 4.5),
                'processing_time': np.random.uniform(0.5, 2.0)
            },
            'model_b': {
                'model_id': model_b_id,
                'accuracy': np.random.uniform(0.85, 0.95),
                'user_satisfaction': np.random.uniform(3.5, 4.5),
                'processing_time': np.random.uniform(0.5, 2.0)
            },
            'test_duration_hours': 24,
            'sample_size': 1000,
            'statistical_significance': 0.05
        }
        
        # Determine winner
        model_a_score = (results['model_a']['accuracy'] * 0.5 + 
                        results['model_a']['user_satisfaction'] / 5.0 * 0.3 +
                        (2.0 - results['model_a']['processing_time']) / 2.0 * 0.2)
        
        model_b_score = (results['model_b']['accuracy'] * 0.5 + 
                        results['model_b']['user_satisfaction'] / 5.0 * 0.3 +
                        (2.0 - results['model_b']['processing_time']) / 2.0 * 0.2)
        
        results['winner'] = model_a_id if model_a_score > model_b_score else model_b_id
        results['score_difference'] = abs(model_a_score - model_b_score)
        
        return results
    
    async def _store_model_performance(self, performance: ModelPerformance) -> None:
        """Store model performance in database."""
        try:
            performance_data = {
                'model_id': performance.model_id,
                'accuracy': performance.accuracy,
                'performance_metrics': {
                    'precision': performance.precision,
                    'recall': performance.recall,
                    'f1_score': performance.f1_score,
                    'auc_roc': performance.auc_roc,
                    'calibration_score': performance.calibration_score
                },
                'validation_date': performance.validation_date
            }
            
            if hasattr(self.database, 'store_model_performance'):
                self.database.store_model_performance(performance_data)
            else:
                self.logger.info(f"Model performance storage not implemented. Performance: {performance.model_id}")
        
        except Exception as e:
            self.logger.error(f"Failed to store model performance: {e}")


# Integration function for Phase 4A testing
async def demonstrate_advanced_classification():
    """Demonstrate advanced classification model capabilities."""
    
    print("ü§ñ Phase 4A: Advanced Classification Models Demonstration")
    print("=" * 70)
    
    # Mock database for demonstration
    class MockDatabase:
        def store_model_performance(self, data):
            print(f"üìä Model performance stored: {data['model_id']} (accuracy: {data['accuracy']:.3f})")
    
    # Initialize model manager
    db = MockDatabase()
    config = {'model_registry_path': '/tmp/model_registry'}
    manager = ModelManager(db, config)
    
    print("üîß Initializing advanced classification models...")
    
    # Create and register transformer model
    transformer_config = {
        'base_model': 'bert-base-uncased',
        'max_length': 512,
        'num_classes': 13,
        'fine_tune_layers': 2
    }
    transformer_model = TransformerClassificationModel("transformer_v1", transformer_config)
    manager.register_model(transformer_model)
    
    # Create and register ensemble model
    ensemble_config = {
        'ensemble_method': 'weighted_voting',
        'base_model_configs': [
            {'type': 'transformer', 'weight': 0.4},
            {'type': 'gradient_boosting', 'weight': 0.3},
            {'type': 'rule_based', 'weight': 0.3}
        ]
    }
    ensemble_model = EnsembleClassificationModel("ensemble_v1", ensemble_config)
    manager.register_model(ensemble_model)
    
    print(f"   ‚úÖ Registered {len(manager.models)} models")
    
    # Generate mock training data
    print("üìö Generating training and validation datasets...")
    training_data = [{'study_id': f'train_{i}', 'label': f'class_{i%6}'} for i in range(1000)]
    validation_data = [{'study_id': f'val_{i}', 'label': f'class_{i%6}'} for i in range(200)]
    
    print(f"   Training set: {len(training_data)} samples")
    print(f"   Validation set: {len(validation_data)} samples")
    
    # Train models
    print("\nüéØ Training models...")
    
    transformer_performance = await manager.train_model("transformer_v1", training_data, validation_data)
    print(f"   ‚úÖ Transformer trained - Accuracy: {transformer_performance.accuracy:.3f}")
    
    ensemble_performance = await manager.train_model("ensemble_v1", training_data, validation_data)
    print(f"   ‚úÖ Ensemble trained - Accuracy: {ensemble_performance.accuracy:.3f}")
    
    # Compare models
    print("\nüìä Comparing model performance...")
    test_data = [{'study_id': f'test_{i}', 'label': f'class_{i%6}'} for i in range(100)]
    
    comparison_results = await manager.compare_models(
        ["transformer_v1", "ensemble_v1"], 
        test_data
    )
    
    print("   Model Comparison Results:")
    for model_id, performance in comparison_results.items():
        print(f"     {model_id}: Accuracy {performance.accuracy:.3f}, Overall Score {performance.get_overall_score():.3f}")
    
    # Select best model
    print("\nüèÜ Selecting best model...")
    best_model_id = await manager.select_best_model(comparison_results)
    print(f"   Selected: {best_model_id}")
    
    # Make predictions with best model
    print("\nüîÆ Making predictions with best model...")
    
    # Create sample features
    sample_features = ClassificationFeatures(
        title_features={'length': 120, 'keywords': ['randomized', 'controlled', 'trial']},
        abstract_features={'length': 1500, 'methodology_terms': ['randomization', 'blinding']},
        author_features={'count': 5, 'institutions': 3},
        journal_features={'impact_factor': 8.5, 'field': 'medicine'},
        metadata_features={'year': 2024, 'country': 'USA'},
        linguistic_features={'readability': 0.7, 'complexity': 0.6},
        semantic_features={'topic_similarity': 0.8, 'method_confidence': 0.9}
    )
    
    prediction = await manager.predict_with_active_model(sample_features)
    
    print(f"   üéØ Prediction Results:")
    print(f"     Study ID: {prediction.study_id}")
    print(f"     Predicted Class: {prediction.predicted_class}")
    print(f"     Confidence: {prediction.confidence_score:.3f} ({prediction.confidence_level.value})")
    print(f"     Top Probabilities:")
    
    sorted_probs = sorted(prediction.class_probabilities.items(), key=lambda x: x[1], reverse=True)
    for class_name, prob in sorted_probs[:3]:
        print(f"       {class_name}: {prob:.3f}")
    
    print(f"   üìù Explanation: {prediction.explanation}")
    
    # Demonstrate A/B testing
    print("\nüî¨ Performing A/B testing...")
    ab_results = await manager.a_b_test_models("transformer_v1", "ensemble_v1")
    
    print(f"   A/B Test Results:")
    print(f"     Test ID: {ab_results['test_id']}")
    print(f"     Winner: {ab_results['winner']}")
    print(f"     Score Difference: {ab_results['score_difference']:.3f}")
    print(f"     Model A (Transformer): Accuracy {ab_results['model_a']['accuracy']:.3f}")
    print(f"     Model B (Ensemble): Accuracy {ab_results['model_b']['accuracy']:.3f}")
    
    # Model registry summary
    print(f"\nüìã Model Registry Summary:")
    for model_id, info in manager.model_registry.items():
        print(f"   {model_id}:")
        print(f"     Type: {info['model_type']}")
        print(f"     Trained: {info['is_trained']}")
        print(f"     Registered: {info['registered_at']}")
    
    print(f"\n‚úÖ Phase 4A Advanced Classification Models demonstration completed!")
    print(f"   Models trained: {len(manager.models)}")
    print(f"   Active model: {manager.active_model_id}")
    print(f"   Best accuracy achieved: {max(p.accuracy for p in [transformer_performance, ensemble_performance]):.3f}")
    
    return manager, comparison_results, ab_results


if __name__ == "__main__":
    import asyncio
    asyncio.run(demonstrate_advanced_classification())
