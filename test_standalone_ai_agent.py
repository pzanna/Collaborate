#!/usr/bin/env python3
"""
Simple AI Agent - Standalone Version for Testing

A simplified implementation of the AI Agent service abstraction
from Architecture.md Phase 2, designed for standalone testing
without complex import dependencies.

This demonstrates:
- Centralized AI service abstraction
- Multi-agent access pattern
- Service-based architecture
- Basic error handling and monitoring
"""

import asyncio
import time
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional


class StandaloneSimpleAIAgent:
    """
    Simple AI Agent implementation for standalone testing.
    
    This class demonstrates the Architecture.md Phase 2 AI service abstraction
    pattern without external dependencies, allowing validation of the core
    architectural concepts.
    """
    
    def __init__(self, config_manager=None):
        """Initialize the Simple AI Agent service."""
        self.config_manager = config_manager
        self.service_id = f"simple_ai_agent_{str(uuid.uuid4())[:8]}"
        self.service_type = "ai_service_abstraction"
        self.version = "2.0.0-phase2"
        self.start_time = time.time()
        self.requests_processed = 0
        
        # Service capabilities as per Architecture.md Phase 2
        self.capabilities = [
            "text_generation",
            "health_monitoring", 
            "service_statistics",
            "multi_agent_access",
            "error_handling"
        ]
        
        # Mock AI providers for demonstration
        self.available_providers = ["openai", "xai"]
        self.ai_available = True
        
        print(f"âœ… {self.service_id} initialized - Phase 2 AI Service Abstraction")
    
    def get_service_info(self) -> Dict[str, Any]:
        """Get service information and status."""
        return {
            "service_id": self.service_id,
            "service_type": self.service_type,
            "version": self.version,
            "capabilities": self.capabilities,
            "description": "Centralized AI service abstraction for multi-agent access",
            "start_time": self.start_time,
            "uptime": time.time() - self.start_time,
            "requests_processed": self.requests_processed,
            "ai_available": self.ai_available,
            "available_providers": self.available_providers
        }
    
    async def generate_text_direct(self, prompt: str) -> str:
        """
        Direct text generation method.
        
        In a full implementation, this would route to AI providers.
        For testing, it returns a mock response demonstrating the pattern.
        """
        if not prompt or prompt.strip() == "":
            raise ValueError("Prompt cannot be empty")
        
        self.requests_processed += 1
        
        # Mock response demonstrating AI service abstraction
        response = (
            f"[AI Service Response] This is a mock response to: '{prompt[:50]}...' "
            f"Generated by {self.service_id} using centralized AI service abstraction. "
            f"In a full implementation, this would be routed through AI providers like "
            f"{', '.join(self.available_providers)} with fallback mechanisms and cost tracking."
        )
        
        return response
    
    async def _process_task_impl(self, task) -> Dict[str, Any]:
        """
        Process a research task through the AI service abstraction.
        
        This method demonstrates the centralized AI access pattern
        where all agents route AI requests through this service.
        """
        start_time = time.time()
        self.requests_processed += 1
        
        # Route based on action
        if task.action == "generate_text":
            return await self._handle_text_generation(task, start_time)
        elif task.action == "health_check":
            return await self._handle_health_check(task, start_time)
        elif task.action == "get_stats":
            return await self._handle_statistics(task, start_time)
        else:
            return {
                "status": "not_implemented",
                "message": f"Action '{task.action}' not implemented in Simple AI Agent",
                "supported_actions": ["generate_text", "health_check", "get_stats"],
                "service_id": self.service_id,
                "response_time": time.time() - start_time
            }
    
    async def _handle_text_generation(self, task, start_time: float) -> Dict[str, Any]:
        """Handle text generation requests."""
        prompt = task.payload.get("prompt", "")
        
        if not prompt:
            raise ValueError("Text generation requires a non-empty prompt")
        
        # Generate response using the direct method
        response = await self.generate_text_direct(prompt)
        
        return {
            "status": "success",
            "response": response,
            "response_length": len(response),
            "method": "simple_ai_agent_mock",
            "service_id": self.service_id,
            "response_time": time.time() - start_time,
            "agent_requesting": task.agent_type
        }
    
    async def _handle_health_check(self, task, start_time: float) -> Dict[str, Any]:
        """Handle health check requests."""
        return {
            "status": "healthy",
            "service_id": self.service_id,
            "uptime": time.time() - self.start_time,
            "requests_processed": self.requests_processed,
            "ai_clients_available": self.ai_available,
            "available_providers": self.available_providers,
            "capabilities": self.capabilities,
            "response_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _handle_statistics(self, task, start_time: float) -> Dict[str, Any]:
        """Handle statistics requests."""
        return {
            "status": "success",
            "service_stats": {
                "service_id": self.service_id,
                "uptime": time.time() - self.start_time,
                "requests_processed": self.requests_processed,
                "average_response_time": 0.1  # Mock value
            },
            "capabilities": self.capabilities,
            "ai_integration_status": {
                "ai_available": self.ai_available,
                "providers_configured": len(self.available_providers),
                "available_providers": self.available_providers
            },
            "response_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }


# Mock classes for testing
class MockConfigManager:
    """Mock config manager for testing."""
    
    def __init__(self):
        self.config = {
            "ai": {
                "openai": {"api_key": "test-key", "model": "gpt-4"},
                "xai": {"api_key": "test-key", "model": "grok-beta"}
            }
        }
    
    def get(self, key, default=None):
        keys = key.split('.')
        value = self.config
        for k in keys:
            value = value.get(k, {})
        return value if value else default


class ResearchAction:
    """Mock ResearchAction for testing."""
    
    def __init__(self, task_id: str, context_id: str, agent_type: str, action: str, payload: Dict[str, Any]):
        self.task_id = task_id
        self.context_id = context_id
        self.agent_type = agent_type
        self.action = action
        self.payload = payload


def create_test_task(action: str, payload: Dict[str, Any], task_id: Optional[str] = None) -> ResearchAction:
    """Create a test task."""
    return ResearchAction(
        task_id=task_id or f"test_{str(uuid.uuid4())[:8]}",
        context_id="test_context",
        agent_type="simple_ai_agent",
        action=action,
        payload=payload
    )


# Test functions
async def test_initialization():
    """Test initialization."""
    print("\nğŸ”§ Testing Simple AI Agent Initialization...")
    
    try:
        config_manager = MockConfigManager()
        ai_agent = StandaloneSimpleAIAgent(config_manager)
        
        service_info = ai_agent.get_service_info()
        print(f"âœ… Service ID: {service_info['service_id']}")
        print(f"âœ… Service Type: {service_info['service_type']}")
        print(f"âœ… Version: {service_info['version']}")
        print(f"âœ… Capabilities: {len(service_info['capabilities'])} features")
        print(f"âœ… Description: {service_info['description']}")
        
        return ai_agent
        
    except Exception as e:
        print(f"âŒ Initialization failed: {e}")
        return None


async def test_text_generation(ai_agent):
    """Test text generation."""
    print("\nğŸ“ Testing Text Generation...")
    
    try:
        # Direct generation
        response = await ai_agent.generate_text_direct(
            "Explain centralized AI service abstraction in software architecture."
        )
        print(f"âœ… Direct generation ({len(response)} chars): {response[:100]}...")
        
        # Via task protocol
        task = create_test_task("generate_text", {
            "prompt": "What are the benefits of AI service abstraction for multi-agent systems?"
        })
        
        result = await ai_agent._process_task_impl(task)
        print(f"âœ… Task response length: {result['response_length']}")
        print(f"âœ… Method used: {result['method']}")
        print(f"âœ… Response time: {result['response_time']:.3f}s")
        print(f"âœ… Requesting agent: {result['agent_requesting']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Text generation failed: {e}")
        return False


async def test_health_check(ai_agent):
    """Test health check."""
    print("\nğŸ¥ Testing Health Check...")
    
    try:
        task = create_test_task("health_check", {})
        result = await ai_agent._process_task_impl(task)
        
        print(f"âœ… Service status: {result['status']}")
        print(f"âœ… Uptime: {result['uptime']:.2f}s")
        print(f"âœ… Requests processed: {result['requests_processed']}")
        print(f"âœ… AI available: {result['ai_clients_available']}")
        print(f"âœ… Providers: {result['available_providers']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Health check failed: {e}")
        return False


async def test_statistics(ai_agent):
    """Test statistics."""
    print("\nğŸ“Š Testing Statistics...")
    
    try:
        task = create_test_task("get_stats", {})
        result = await ai_agent._process_task_impl(task)
        
        service_stats = result['service_stats']
        print(f"âœ… Service uptime: {service_stats['uptime']:.2f}s")
        print(f"âœ… Requests processed: {service_stats['requests_processed']}")
        print(f"âœ… Capabilities: {result['capabilities']}")
        
        ai_status = result['ai_integration_status']
        print(f"âœ… Providers configured: {ai_status['providers_configured']}")
        print(f"âœ… Available providers: {ai_status['available_providers']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Statistics failed: {e}")
        return False


async def test_multi_agent_access(ai_agent):
    """Test multi-agent access pattern."""
    print("\nğŸ—ï¸ Testing Multi-Agent Access Pattern...")
    
    try:
        # Simulate different agent types accessing the AI service
        agent_types = ["literature_agent", "planning_agent", "executor_agent", "memory_agent"]
        
        for agent_type in agent_types:
            task = create_test_task("generate_text", {
                "prompt": f"This is a request from the {agent_type} for AI services."
            })
            task.agent_type = agent_type
            
            result = await ai_agent._process_task_impl(task)
            print(f"âœ… {agent_type}: got {result['response_length']} char response")
        
        # Verify service processed all requests
        final_stats = ai_agent.get_service_info()
        print(f"âœ… Total requests processed: {final_stats['requests_processed']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Multi-agent access test failed: {e}")
        return False


async def test_error_handling(ai_agent):
    """Test error handling."""
    print("\nâš ï¸ Testing Error Handling...")
    
    try:
        # Test unsupported action
        task = create_test_task("unsupported_action", {})
        result = await ai_agent._process_task_impl(task)
        
        if result['status'] == 'not_implemented':
            print("âœ… Correctly handled unsupported action")
            print(f"âœ… Supported actions: {result['supported_actions']}")
        else:
            print("âŒ Should have returned not_implemented status")
            return False
        
        # Test empty prompt
        try:
            await ai_agent.generate_text_direct("")
            print("âŒ Should have raised error for empty prompt")
            return False
        except ValueError as e:
            print(f"âœ… Correctly handled empty prompt: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error handling test failed: {e}")
        return False


async def main():
    """Run the standalone test suite."""
    print("ğŸš€ Simple AI Agent Standalone Test - Architecture.md Phase 2")
    print("=" * 70)
    
    # Initialize
    ai_agent = await test_initialization()
    if not ai_agent:
        print("\nâŒ Test suite failed at initialization")
        return False
    
    # Test suite
    tests = [
        test_text_generation,
        test_health_check,
        test_statistics,
        test_multi_agent_access,
        test_error_handling
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if await test_func(ai_agent):
                passed += 1
        except Exception as e:
            print(f"âŒ Test {test_func.__name__} crashed: {e}")
    
    # Final results
    print("\n" + "=" * 70)
    print(f"ğŸ Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ ALL TESTS PASSED! Simple AI Agent is working correctly.")
        print("\nğŸ“‹ Architecture.md Phase 2 Compliance Verified:")
        print("  âœ… Centralized AI service abstraction implemented")
        print("  âœ… Multi-agent access pattern demonstrated")
        print("  âœ… Service-based architecture pattern working")
        print("  âœ… Error handling and monitoring functional")
        print("  âœ… Unified interface for AI provider access")
        print("  âœ… Basic statistics and health monitoring")
        print("\nğŸ”„ Ready for Phase 2 continuation!")
        return True
    else:
        print("âš ï¸ Some tests failed. Check the output above for details.")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Tests interrupted by user")
        exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Test suite crashed: {e}")
        exit(1)
